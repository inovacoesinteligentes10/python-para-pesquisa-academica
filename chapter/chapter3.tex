\chapter{Bibliotecas Científicas Essenciais: NumPy, Pandas e Matplotlib}

Embora Python puro seja poderoso, o verdadeiro potencial para pesquisa acadêmica surge quando combinamos a linguagem com suas bibliotecas científicas especializadas. Este capítulo explora as três bibliotecas fundamentais que formam a espinha dorsal de praticamente qualquer projeto de análise de dados: NumPy para computação numérica, pandas para manipulação de dados estruturados, e matplotlib para visualização.

\section{NumPy: Computação Numérica Eficiente}

NumPy (Numerical Python) é a base de todo o ecossistema científico Python. Ele fornece arrays multidimensionais eficientes e operações matemáticas otimizadas que são ordens de magnitude mais rápidas que listas Python puras.

\subsection{Por que NumPy é Crucial para Pesquisa}

A diferença de performance entre listas Python e arrays NumPy pode ser a diferença entre uma análise que leva minutos versus horas, especialmente com datasets grandes comuns em pesquisa moderna.

\begin{examplebox}
\textbf{Comparação de Performance: Python vs NumPy}

\begin{lstlisting}[language=Python]
import numpy as np
import time

# Dados de exemplo: medicoes de 1 milhao de participantes
n = 1000000

# Metodo 1: Listas Python puras
dados_python = list(range(n))
inicio = time.time()
resultado_python = [x**2 for x in dados_python]
tempo_python = time.time() - inicio

# Metodo 2: Arrays NumPy
dados_numpy = np.arange(n)
inicio = time.time()
resultado_numpy = dados_numpy**2
tempo_numpy = time.time() - inicio

print(f"Tempo Python puro: {tempo_python:.3f} segundos")
print(f"Tempo NumPy: {tempo_numpy:.3f} segundos")
print(f"NumPy e {tempo_python/tempo_numpy:.1f}x mais rapido")

# Verificar se resultados sao iguais
print(f"Resultados identicos: {np.array_equal(resultado_python, resultado_numpy)}")
\end{lstlisting}
\end{examplebox}

\subsection{Arrays NumPy: Fundamentos}

Arrays NumPy são estruturas homogêneas (todos elementos do mesmo tipo) que permitem operações vetorizadas eficientes.

\begin{examplebox}
\textbf{Criação e Manipulação Básica de Arrays}

\begin{lstlisting}[language=Python]
import numpy as np

# Diferentes maneiras de criar arrays
dados_experimentais = np.array([1.2, 1.5, 1.3, 1.7, 1.1])
zeros = np.zeros(10)  # Array de zeros
uns = np.ones((3, 4))  # Matriz 3x4 de uns
sequencia = np.arange(0, 10, 0.5)  # De 0 a 10 com passo 0.5
linear = np.linspace(0, 100, 50)  # 50 pontos igualmente espaçados

# Propriedades importantes
print(f"Forma: {dados_experimentais.shape}")
print(f"Tipo de dados: {dados_experimentais.dtype}")
print(f"Numero de dimensoes: {dados_experimentais.ndim}")
print(f"Tamanho total: {dados_experimentais.size}")

# Operacoes basicas (vetorizadas)
print(f"Media: {np.mean(dados_experimentais):.3f}")
print(f"Desvio padrao: {np.std(dados_experimentais):.3f}")
print(f"Maximo: {np.max(dados_experimentais):.3f}")
print(f"Minimo: {np.min(dados_experimentais):.3f}")

# Operacoes elemento a elemento
dados_normalizados = (dados_experimentais - np.mean(dados_experimentais)) / np.std(dados_experimentais)
print(f"Dados normalizados: {dados_normalizados}")
\end{lstlisting}
\end{examplebox}

\subsection{Arrays Multidimensionais para Dados Complexos}

Dados de pesquisa frequentemente têm múltiplas dimensões: participantes × condições × medições temporais.

\begin{researchbox}
\textbf{Exemplo: Análise de Dados de EEG}

\begin{lstlisting}[language=Python]
# Simular dados de EEG: 64 eletrodos, 1000 pontos temporais, 100 trials
n_eletrodos = 64
n_tempos = 1000
n_trials = 100

# Criar dados simulados (normalmente carregados de arquivo)
np.random.seed(42)
dados_eeg = np.random.randn(n_eletrodos, n_tempos, n_trials)

print(f"Forma dos dados: {dados_eeg.shape}")
print(f"Memoria ocupada: {dados_eeg.nbytes / 1024**2:.1f} MB")

# Calcular media por eletrodo atraves dos trials
media_eletrodos = np.mean(dados_eeg, axis=2)  # media na dimensao trials
print(f"Forma da media: {media_eletrodos.shape}")

# Encontrar eletrodo com maior atividade media
atividade_media = np.mean(np.abs(media_eletrodos), axis=1)
eletrodo_max = np.argmax(atividade_media)
print(f"Eletrodo mais ativo: {eletrodo_max} (atividade: {atividade_media[eletrodo_max]:.3f})")
\end{lstlisting}
\end{researchbox}

\begin{researchbox}
\textbf{Continuação: Análise de Dados de EEG}

\begin{lstlisting}[language=Python]
# Calcular potencial evocado (media atraves de trials)
potencial_evocado = np.mean(dados_eeg, axis=2)

# Encontrar pico máximo no tempo
tempo_max = np.unravel_index(np.argmax(np.abs(potencial_evocado)), 
                            potencial_evocado.shape)
print(f"Pico maximo no eletrodo {tempo_max[0]}, tempo {tempo_max[1]}")
\end{lstlisting}
\end{researchbox}

\subsection{Indexação Avançada e Máscaras Booleanas}

Uma das características mais poderosas do NumPy é a capacidade de filtrar e selecionar dados baseado em condições complexas.

\begin{examplebox}
\textbf{Filtragem Avançada de Dados Experimentais - Parte 1}

\begin{lstlisting}[language=Python]
# Dados de um experimento: tempo de reacao e acuracia
n_participantes = 1000
np.random.seed(123)

tempos_reacao = np.random.normal(500, 100, n_participantes)  # ms
acuracia = np.random.beta(8, 2, n_participantes)  # proporção entre 0-1
idades = np.random.randint(18, 65, n_participantes)
grupos = np.random.choice(['controle', 'experimental'], n_participantes)

# Criar mascara booleana para participantes validos
# (tempo de reacao entre 200-1000ms e acuracia > 50%)
mascara_validos = (tempos_reacao >= 200) & (tempos_reacao <= 1000) & (acuracia > 0.5)

print(f"Participantes totais: {n_participantes}")
print(f"Participantes validos: {np.sum(mascara_validos)}")
print(f"Taxa de exclusao: {(1 - np.mean(mascara_validos))*100:.1f}%")

# Aplicar filtros
tempos_validos = tempos_reacao[mascara_validos]
acuracia_valida = acuracia[mascara_validos]
idades_validas = idades[mascara_validos]
grupos_validos = grupos[mascara_validos]
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Filtragem Avançada de Dados Experimentais - Parte 2}

\begin{lstlisting}[language=Python]
# Aalise por grupo usando máscaras
mascara_controle = grupos_validos == 'controle'
mascara_experimental = grupos_validos == 'experimental'

print(f"\nGrupo Controle (n={np.sum(mascara_controle)}):")
print(f"  Tempo reacao medio: {np.mean(tempos_validos[mascara_controle]):.1f} ms")
print(f"  Acuracia media: {np.mean(acuracia_valida[mascara_controle])*100:.1f}%")

print(f"\nGrupo Experimental (n={np.sum(mascara_experimental)}):")
print(f"  Tempo reacao medio: {np.mean(tempos_validos[mascara_experimental]):.1f} ms")
print(f"  Acuracia media: {np.mean(acuracia_valida[mascara_experimental])*100:.1f}%")

# Analise por faixa etária
jovens = idades_validas < 30
adultos = (idades_validas >= 30) & (idades_validas < 50)
idosos = idades_validas >= 50

print(f"\nPor faixa etaria:")
for nome, mascara in [('Jovens', jovens), ('Adultos', adultos), ('Idosos', idosos)]:
    if np.sum(mascara) > 0:
        print(f"  {nome}: TR = {np.mean(tempos_validos[mascara]):.1f} ms")
\end{lstlisting}
\end{examplebox}

\section{Pandas: Manipulação de Dados Estruturados}

Pandas é construído sobre NumPy mas oferece estruturas de dados mais flexíveis e intuitivas para trabalhar com dados heterogêneos, como planilhas e bancos de dados.

\subsection{DataFrames: Planilhas Inteligentes}

DataFrames são a estrutura central do pandas, similares a planilhas Excel mas com capacidades muito mais avançadas.

\begin{examplebox}
\textbf{Criação e Exploração Básica de DataFrames}

\begin{lstlisting}[language=Python]
import pandas as pd
import numpy as np

# Criar DataFrame a partir de dicionario
dados_participantes = {
    'id': range(1, 101),
    'idade': np.random.randint(18, 65, 100),
    'genero': np.random.choice(['M', 'F'], 100),
    'grupo': np.random.choice(['controle', 'tratamento'], 100),
    'pre_teste': np.random.normal(50, 10, 100),
    'pos_teste': np.random.normal(55, 12, 100),
    'satisfacao': np.random.randint(1, 11, 100)
}

df = pd.DataFrame(dados_participantes)

# Exploracao inicial
print("Informacoes basicas:")
print(df.info())
print(f"\nForma: {df.shape}")
print(f"\nPrimeiras 5 linhas:")
print(df.head())

print(f"\nEstatisticas descritivas:")
print(df.describe())

# Verificar valores ausentes
print(f"\nValores ausentes por coluna:")
print(df.isnull().sum())
\end{lstlisting}
\end{examplebox}

\subsection{Operações de Agrupamento e Análise}

Pandas excela em operações group-by que são fundamentais para análise experimental.

\begin{researchbox}
\textbf{Análise de Eficácia de Tratamento - Parte 1}

\begin{lstlisting}[language=Python]
# Calcular melhoria (diferença pré-pós teste)
df['melhoria'] = df['pos_teste'] - df['pre_teste']

# Analise por grupo
analise_grupo = df.groupby('grupo').agg({
    'idade': ['mean', 'std', 'count'],
    'pre_teste': ['mean', 'std'],
    'pos_teste': ['mean', 'std'],
    'melhoria': ['mean', 'std'],
    'satisfacao': ['mean', 'std']
}).round(2)

print("Análise por grupo:")
print(analise_grupo)

# Análise mais detalhada com múltiplas variáveis
analise_detalhada = df.groupby(['grupo', 'genero']).agg({
    'melhoria': ['count', 'mean', 'std'],
    'satisfacao': 'mean'
}).round(2)

print(f"\nAnálise por grupo e gênero:")
print(analise_detalhada)
\end{lstlisting}
\end{researchbox}

\begin{researchbox}
\textbf{Análise de Eficácia de Tratamento - Parte 2}

\begin{lstlisting}[language=Python]
# Calcular tamanho do efeito simples
def cohen_d(grupo1, grupo2):
    """Calcula Cohen's d entre dois grupos"""
    n1, n2 = len(grupo1), len(grupo2)
    s1, s2 = grupo1.std(), grupo2.std()
    
    # Pooled standard deviation
    s_pooled = np.sqrt(((n1-1)*s1**2 + (n2-1)*s2**2) / (n1+n2-2))
    
    return (grupo1.mean() - grupo2.mean()) / s_pooled

controle = df[df['grupo'] == 'controle']['melhoria']
tratamento = df[df['grupo'] == 'tratamento']['melhoria']

d = cohen_d(tratamento, controle)
print(f"\nTamanho do efeito (Cohen's d): {d:.3f}")

# Interpretacao
if abs(d) < 0.2:
    interpretacao = "trivial"
elif abs(d) < 0.5:
    interpretacao = "pequeno"
elif abs(d) < 0.8:
    interpretacao = "médio"
else:
    interpretacao = "grande"

print(f"Interpretação: efeito {interpretacao}")
\end{lstlisting}
\end{researchbox}

\subsection{Limpeza e Transformação de Dados}

Dados reais sempre precisam de limpeza. Pandas oferece ferramentas poderosas para isso.

\begin{examplebox}
\textbf{Pipeline Completo de Limpeza de Dados - Parte 1}

\begin{lstlisting}[language=Python]
# Simular dados com problemas comuns
np.random.seed(42)
dados_sujos = pd.DataFrame({
    'participante_id': range(1, 201),
    'idade': np.random.randint(16, 80, 200),
    'score': np.random.normal(75, 15, 200),
    'grupo': np.random.choice(['A', 'B', 'C'], 200),
    'data_coleta': pd.date_range('2024-01-01', periods=200, freq='D')
})

# Introduzir problemas nos dados
# 1. Valores ausentes
indices_na = np.random.choice(200, 20, replace=False)
dados_sujos.loc[indices_na, 'score'] = np.nan

# 2. Outliers extremos
indices_outliers = np.random.choice(200, 5, replace=False)
dados_sujos.loc[indices_outliers, 'score'] = [200, -50, 300, -100, 250]

# 3. Idades impossíveis
dados_sujos.loc[190:195, 'idade'] = [150, 5, 200, 0, -10]

print("Dados antes da limpeza:")
print(f"Valores ausentes: {dados_sujos.isnull().sum().sum()}")
print(f"Forma: {dados_sujos.shape}")
print(f"\nEstatísticas do score:")
print(dados_sujos['score'].describe())
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Pipeline Completo de Limpeza de Dados - Parte 2}

\begin{lstlisting}[language=Python]
# Pipeline de limpeza
dados_limpos = dados_sujos.copy()

# 1. Filtrar idades válidas (18-65)
mask_idade_valida = (dados_limpos['idade'] >= 18) & (dados_limpos['idade'] <= 65)
dados_limpos = dados_limpos[mask_idade_valida]

# 2. Remover outliers extremos do score (fora de 3 desvios padrão)
mean_score = dados_limpos['score'].mean()
std_score = dados_limpos['score'].std()
limite_inferior = mean_score - 3 * std_score
limite_superior = mean_score + 3 * std_score

mask_score_valido = (dados_limpos['score'] >= limite_inferior) & \
                   (dados_limpos['score'] <= limite_superior)
dados_limpos = dados_limpos[mask_score_valido | dados_limpos['score'].isnull()]

# 3. Tratar valores ausentes - Imputar pela média do grupo
dados_limpos['score'] = dados_limpos.groupby('grupo')['score'].transform(
    lambda x: x.fillna(x.mean())
)
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Pipeline Completo de Limpeza de Dados - Parte 3}

\begin{lstlisting}[language=Python]
# 4. Criar variáveis derivadas
dados_limpos['faixa_etaria'] = pd.cut(dados_limpos['idade'], 
                                     bins=[0, 30, 50, 100], 
                                     labels=['Jovem', 'Adulto', 'Idoso'])

dados_limpos['score_categoria'] = pd.cut(dados_limpos['score'],
                                        bins=[0, 60, 80, 100],
                                        labels=['Baixo', 'Médio', 'Alto'])

print(f"\nDados após limpeza:")
print(f"Valores ausentes: {dados_limpos.isnull().sum().sum()}")
print(f"Forma: {dados_limpos.shape}")
print(f"Registros removidos: {len(dados_sujos) - len(dados_limpos)}")

print(f"\nDistribuição por faixa etária:")
print(dados_limpos['faixa_etaria'].value_counts())

print(f"\nDistribuição por categoria de score:")
print(dados_limpos['score_categoria'].value_counts())
\end{lstlisting}
\end{examplebox}

\section{Matplotlib: Visualização Científica}

Visualização é crucial para comunicar resultados de pesquisa. Matplotlib oferece controle completo sobre cada aspecto dos gráficos.

\subsection{Gráficos Básicos para Publicação}

\begin{examplebox}
\textbf{Gráficos Prontos para Publicação - Configuração Inicial}

\begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar estilo para publicação
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams.update({
    'font.size': 12,
    'axes.labelsize': 14,
    'axes.titlesize': 16,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.titlesize': 18
})

# Dados para visualização
grupos = ['Controle', 'Tratamento A', 'Tratamento B']
pre_teste = [72.5, 73.1, 72.8]
pos_teste = [74.2, 81.3, 85.7]
erros_pre = [2.1, 2.3, 2.0]
erros_pos = [2.5, 2.8, 3.1]

# Criar figura com múltiplos subplots
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Gráficos Prontos para Publicação - Gráfico de Barras}

\begin{lstlisting}[language=Python]
# 1. Gráfico de barras com barras de erro
x_pos = np.arange(len(grupos))
largura = 0.35

barras1 = ax1.bar(x_pos - largura/2, pre_teste, largura, 
                  yerr=erros_pre, label='Pré-teste', alpha=0.8)
barras2 = ax1.bar(x_pos + largura/2, pos_teste, largura,
                  yerr=erros_pos, label='Pós-teste', alpha=0.8)

ax1.set_xlabel('Grupos')
ax1.set_ylabel('Score Médio')
ax1.set_title('Comparação Pré vs Pós-teste')
ax1.set_xticks(x_pos)
ax1.set_xticklabels(grupos)
ax1.legend()
ax1.set_ylim(65, 90)

# Adicionar valores nas barras
for barra in barras1:
    altura = barra.get_height()
    ax1.text(barra.get_x() + barra.get_width()/2., altura + 1,
             f'{altura:.1f}', ha='center', va='bottom')

for barra in barras2:
    altura = barra.get_height()
    ax1.text(barra.get_x() + barra.get_width()/2., altura + 1,
             f'{altura:.1f}', ha='center', va='bottom')
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Gráficos Prontos para Publicação - Boxplot e Outros}

\begin{lstlisting}[language=Python]
# 2. Boxplot
dados_boxplot = [
    np.random.normal(72.5, 8, 50),  # Controle
    np.random.normal(81.3, 9, 50),  # Tratamento A
    np.random.normal(85.7, 7, 50)   # Tratamento B
]

bp = ax2.boxplot(dados_boxplot, labels=grupos, patch_artist=True)
ax2.set_ylabel('Score')
ax2.set_title('Distribuição dos Scores por Grupo')

# Colorir boxplots
cores = ['lightblue', 'lightgreen', 'lightcoral']
for patch, cor in zip(bp['boxes'], cores):
    patch.set_facecolor(cor)

# 3. Gráfico de linha temporal
tempos = np.arange(0, 11)  # 0 a 10 semanas
grupo_controle = 72 + 0.2 * tempos + np.random.normal(0, 1, len(tempos))
grupo_trat = 72 + 1.5 * tempos + np.random.normal(0, 1.5, len(tempos))

ax3.plot(tempos, grupo_controle, 'o-', label='Controle', linewidth=2, markersize=6)
ax3.plot(tempos, grupo_trat, 's-', label='Tratamento', linewidth=2, markersize=6)
ax3.set_xlabel('Semanas')
ax3.set_ylabel('Score')
ax3.set_title('Evolução Temporal')
ax3.legend()
ax3.grid(True, alpha=0.3)
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Gráficos Prontos para Publicação - Scatter Plot Final}

\begin{lstlisting}[language=Python]
# 4. Scatter plot com linha de tendência
np.random.seed(42)
idade = np.random.randint(20, 60, 100)
score = 60 + 0.5 * idade + np.random.normal(0, 5, 100)

ax4.scatter(idade, score, alpha=0.6, s=50)

# Adicionar linha de tendência
z = np.polyfit(idade, score, 1)
p = np.poly1d(z)
ax4.plot(idade, p(idade), "r--", alpha=0.8, linewidth=2)

# Calcular e mostrar correlação
correlacao = np.corrcoef(idade, score)[0, 1]
ax4.text(0.05, 0.95, f'r = {correlacao:.3f}', transform=ax4.transAxes,
         bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))

ax4.set_xlabel('Idade')
ax4.set_ylabel('Score')
ax4.set_title('Relação Idade vs Performance')

plt.tight_layout()
plt.savefig('analise_completa.png', dpi=300, bbox_inches='tight')
plt.show()
\end{lstlisting}
\end{examplebox}

\subsection{Visualizações Avançadas para Pesquisa}

\begin{researchbox}
\textbf{Heatmap de Correlações - Preparação dos Dados}

\begin{lstlisting}[language=Python]
# Criar dataset multivariado simulado
np.random.seed(123)
n_participantes = 200

# Variáveis correlacionadas para simular dados reais
dados_pesquisa = pd.DataFrame({
    'idade': np.random.randint(18, 65, n_participantes),
    'educacao_anos': np.random.randint(8, 20, n_participantes),
    'renda': np.random.normal(50000, 15000, n_participantes),
    'stress_percebido': np.random.randint(1, 11, n_participantes),
    'satisfacao_vida': np.random.randint(1, 11, n_participantes),
    'horas_exercicio': np.random.exponential(3, n_participantes),
    'qualidade_sono': np.random.randint(1, 11, n_participantes)
})

# Introduzir correlações realistas
dados_pesquisa['renda'] += dados_pesquisa['educacao_anos'] * 2000
dados_pesquisa['satisfacao_vida'] = 10 - dados_pesquisa['stress_percebido'] + \
                                   np.random.normal(0, 1, n_participantes)
dados_pesquisa['qualidade_sono'] = 10 - dados_pesquisa['stress_percebido'] * 0.5 + \
                                  dados_pesquisa['horas_exercicio'] * 0.3 + \
                                  np.random.normal(0, 1, n_participantes)

# Limitar valores aos ranges apropriados
dados_pesquisa['satisfacao_vida'] = np.clip(dados_pesquisa['satisfacao_vida'], 1, 10)
dados_pesquisa['qualidade_sono'] = np.clip(dados_pesquisa['qualidade_sono'], 1, 10)
dados_pesquisa['horas_exercicio'] = np.clip(dados_pesquisa['horas_exercicio'], 0, 15)
\end{lstlisting}
\end{researchbox}

\begin{researchbox}
\textbf{Heatmap de Correlações - Visualizações 1-3}

\begin{lstlisting}[language=Python]
# Criar visualização complexa
fig = plt.figure(figsize=(16, 12))

# 1. Heatmap de correlações
ax1 = plt.subplot(2, 3, 1)
correlacoes = dados_pesquisa.corr()
mask = np.triu(np.ones_like(correlacoes, dtype=bool))
sns.heatmap(correlacoes, mask=mask, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .8})
plt.title('Matriz de Correlações')

# 2. Distribuição de idades por quartis de satisfação
ax2 = plt.subplot(2, 3, 2)
dados_pesquisa['quartil_satisfacao'] = pd.qcut(dados_pesquisa['satisfacao_vida'], 
                                              4, labels=['Q1', 'Q2', 'Q3', 'Q4'])
dados_pesquisa.boxplot(column='idade', by='quartil_satisfacao', ax=ax2)
plt.title('Idade por Quartil de Satisfação')
plt.suptitle('')  # Remove título automático do pandas

# 3. Scatter plot matriz
ax3 = plt.subplot(2, 3, 3)
scatter = ax3.scatter(dados_pesquisa['stress_percebido'], 
                     dados_pesquisa['qualidade_sono'],
                     c=dados_pesquisa['horas_exercicio'], 
                     s=dados_pesquisa['idade'],
                     alpha=0.6, cmap='viridis')
ax3.set_xlabel('Stress Percebido')
ax3.set_ylabel('Qualidade do Sono')
ax3.set_title('Stress vs Sono (cor=exercício, tamanho=idade)')
cbar = plt.colorbar(scatter, ax=ax3)
cbar.set_label('Horas de Exercício')
\end{lstlisting}
\end{researchbox}

\begin{researchbox}
\textbf{Heatmap de Correlações - Visualizações 4-6}

\begin{lstlisting}[language=Python]
# 4. Histograma de distribuições
ax4 = plt.subplot(2, 3, 4)
dados_pesquisa[['stress_percebido', 'satisfacao_vida', 'qualidade_sono']].hist(
    bins=10, alpha=0.7, ax=ax4)
plt.title('Distribuições das Variáveis Principais')

# 5. Gráfico de violin
ax5 = plt.subplot(2, 3, 5)
# Categorizar exercício em grupos
dados_pesquisa['grupo_exercicio'] = pd.cut(dados_pesquisa['horas_exercicio'],
                                          bins=3, labels=['Baixo', 'Médio', 'Alto'])
sns.violinplot(data=dados_pesquisa, x='grupo_exercicio', y='satisfacao_vida', ax=ax5)
plt.title('Satisfação por Nível de Exercício')

# 6. Regressão linear
ax6 = plt.subplot(2, 3, 6)
sns.regplot(data=dados_pesquisa, x='educacao_anos', y='renda', ax=ax6, scatter_kws={'alpha':0.6})
plt.title('Educação vs Renda')

plt.tight_layout()
plt.savefig('analise_multivariada.png', dpi=300, bbox_inches='tight')
plt.show()
\end{lstlisting}
\end{researchbox}

\begin{researchbox}
\textbf{Heatmap de Correlações - Relatório Final}

\begin{lstlisting}[language=Python]
# Relatório estatístico
print("RELATÓRIO DE ANÁLISE MULTIVARIADA")
print("="*50)
print(f"N = {len(dados_pesquisa)} participantes")
print(f"\nCorrelações mais fortes:")
correlacoes_abs = correlacoes.abs()
np.fill_diagonal(correlacoes_abs.values, 0)
maior_corr = correlacoes_abs.stack().nlargest(3)
for i, (vars, corr) in enumerate(maior_corr.items(), 1):
    print(f"{i}. {vars[0]} vs {vars[1]}: r = {correlacoes.loc[vars[0], vars[1]]:.3f}")
\end{lstlisting}
\end{researchbox}

\section{Integração das Três Bibliotecas}

O verdadeiro poder surge quando combinamos NumPy, pandas e Matplotlib em um workflow integrado.

\begin{examplebox}
\textbf{Workflow Completo - Configuração e Coleta}

\begin{lstlisting}[language=Python]
# Simulação de pipeline completo de pesquisa
def pipeline_completo_pesquisa():
    """
    Demonstra workflow típico: dados -> limpeza -> análise -> visualização
    """
    print("PIPELINE COMPLETO DE ANÁLISE DE PESQUISA")
    print("="*50)
    
    # 1. COLETA DE DADOS (simulada)
    print("1. Coletando dados...")
    np.random.seed(42)
    
    dados_brutos = pd.DataFrame({
        'participante': range(1, 301),
        'grupo': np.random.choice(['controle', 'experimental'], 300),
        'pre_teste': np.random.normal(50, 10, 300),
        'pos_teste': np.random.normal(55, 12, 300),
        'idade': np.random.randint(18, 65, 300),
        'genero': np.random.choice(['M', 'F'], 300),
        'tempo_reacao': np.random.lognormal(6, 0.3, 300)  # Distribuição realista
    })
    
    # Introduzir efeito realista do tratamento
    mask_experimental = dados_brutos['grupo'] == 'experimental'
    dados_brutos.loc[mask_experimental, 'pos_teste'] += np.random.normal(8, 3, mask_experimental.sum())
    
    print(f"   Dados coletados: {len(dados_brutos)} participantes")
    return dados_brutos
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Workflow Completo - Limpeza e Análise}

\begin{lstlisting}[language=Python]
def pipeline_limpeza_analise(dados_brutos):
    # 2. LIMPEZA DE DADOS
    print("2. Limpando dados...")
    
    # Remover outliers extremos no tempo de reação (> 3 DP)
    tr_mean = dados_brutos['tempo_reacao'].mean()
    tr_std = dados_brutos['tempo_reacao'].std()
    mask_tr_valido = np.abs(dados_brutos['tempo_reacao'] - tr_mean) <= 3 * tr_std
    
    dados_limpos = dados_brutos[mask_tr_valido].copy()
    print(f"   Outliers removidos: {len(dados_brutos) - len(dados_limpos)}")
    
    # 3. ANALISE ESTATISTICA
    print("3. Realizando análises...")
    
    # Calcular melhoria
    dados_limpos['melhoria'] = dados_limpos['pos_teste'] - dados_limpos['pre_teste']
    
    # Analise por grupo
    resultados_grupo = dados_limpos.groupby('grupo')['melhoria'].agg(['count', 'mean', 'std'])
    print(f"   Resultados por grupo:")
    print(resultados_grupo)
    
    return dados_limpos, resultados_grupo
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Workflow Completo - Testes Estatísticos}

\begin{lstlisting}[language=Python]
def pipeline_testes_estatisticos(dados_limpos):
    # Teste t
    from scipy import stats
    controle = dados_limpos[dados_limpos['grupo'] == 'controle']['melhoria']
    experimental = dados_limpos[dados_limpos['grupo'] == 'experimental']['melhoria']
    
    t_stat, p_valor = stats.ttest_ind(experimental, controle)
    
    # Tamanho do efeito
    def cohen_d(grupo1, grupo2):
        n1, n2 = len(grupo1), len(grupo2)
        s_pooled = np.sqrt(((n1-1)*grupo1.var() + (n2-1)*grupo2.var()) / (n1+n2-2))
        return (grupo1.mean() - grupo2.mean()) / s_pooled
    
    d = cohen_d(experimental, controle)
    
    print(f"   Teste t: t = {t_stat:.3f}, p = {p_valor:.3f}")
    print(f"   Cohen's d = {d:.3f}")
    
    return controle, experimental, t_stat, p_valor, d
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Workflow Completo - Visualizações 1-2}

\begin{lstlisting}[language=Python]
def pipeline_visualizacoes_1_2(dados_limpos, controle, experimental):
    # 4. VISUALIZACAO
    print("4. Criando visualizações...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))
    
    # Grafico 1: Boxplot da melhoria por grupo
    dados_limpos.boxplot(column='melhoria', by='grupo', ax=ax1)
    ax1.set_title('Melhoria por Grupo')
    ax1.set_xlabel('Grupo')
    ax1.set_ylabel('Melhoria (pontos)')
    
    # Gráfico 2: Histograma sobreposto
    ax2.hist(controle, alpha=0.7, label='Controle', bins=20)
    ax2.hist(experimental, alpha=0.7, label='Experimental', bins=20)
    ax2.set_xlabel('Melhoria')
    ax2.set_ylabel('Frequência')
    ax2.set_title('Distribuição da Melhoria')
    ax2.legend()
    
    return fig, ax1, ax2, ax3, ax4
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Workflow Completo - Visualizações 3-4}

\begin{lstlisting}[language=Python]
def pipeline_visualizacoes_3_4(dados_limpos, ax3, ax4):
    # Gráfico 3: Pré vs Pós por grupo
    grupos = ['Controle', 'Experimental']
    pre_medias = [dados_limpos[dados_limpos['grupo'] == g]['pre_teste'].mean() 
                  for g in ['controle', 'experimental']]
    pos_medias = [dados_limpos[dados_limpos['grupo'] == g]['pos_teste'].mean() 
                  for g in ['controle', 'experimental']]
    
    x = np.arange(len(grupos))
    largura = 0.35
    
    ax3.bar(x - largura/2, pre_medias, largura, label='Pré-teste', alpha=0.8)
    ax3.bar(x + largura/2, pos_medias, largura, label='Pós-teste', alpha=0.8)
    ax3.set_xlabel('Grupo')
    ax3.set_ylabel('Score Médio')
    ax3.set_title('Pré vs Pós-teste')
    ax3.set_xticks(x)
    ax3.set_xticklabels(grupos)
    ax3.legend()
    
    # Gráfico 4: Correlação idade vs melhoria
    ax4.scatter(dados_limpos['idade'], dados_limpos['melhoria'], alpha=0.6)
    
    # Linha de tendência
    z = np.polyfit(dados_limpos['idade'], dados_limpos['melhoria'], 1)
    p = np.poly1d(z)
    ax4.plot(dados_limpos['idade'], p(dados_limpos['idade']), "r--", alpha=0.8)
    
    # Correlação
    r = np.corrcoef(dados_limpos['idade'], dados_limpos['melhoria'])[0, 1]
    ax4.text(0.05, 0.95, f'r = {r:.3f}', transform=ax4.transAxes,
             bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
    
    ax4.set_xlabel('Idade')
    ax4.set_ylabel('Melhoria')
    ax4.set_title('Idade vs Melhoria')
    
    plt.tight_layout()
    plt.savefig('pipeline_completo.png', dpi=300, bbox_inches='tight')
    plt.show()
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Workflow Completo - Relatório Final e Execução}

\begin{lstlisting}[language=Python]
def pipeline_relatorio_final(dados_limpos, controle, experimental, t_stat, p_valor, d):
    # 5. RELATÓRIO FINAL
    print("\n5. RELATÓRIO FINAL")
    print("="*30)
    print(f"Amostra final: N = {len(dados_limpos)}")
    print(f"Melhoria Controle: M = {controle.mean():.2f}, DP = {controle.std():.2f}")
    print(f"Melhoria Experimental: M = {experimental.mean():.2f}, DP = {experimental.std():.2f}")
    print(f"Teste t: t({len(dados_limpos)-2}) = {t_stat:.3f}, p = {p_valor:.3f}")
    print(f"Tamanho do efeito: d = {d:.3f}")
    
    if p_valor < 0.05:
        significancia = "significativa"
    else:
        significancia = "não significativa"
    
    if abs(d) < 0.2:
        interpretacao_d = "trivial"
    elif abs(d) < 0.5:
        interpretacao_d = "pequeno"
    elif abs(d) < 0.8:
        interpretacao_d = "médio"
    else:
        interpretacao_d = "grande"
    
    print(f"\nCONCLUSÃO: Diferença {significancia} com efeito {interpretacao_d}")
    
    return dados_limpos

# Executar pipeline completo
dados_brutos = pipeline_completo_pesquisa()
dados_limpos, resultados = pipeline_limpeza_analise(dados_brutos)
controle, experimental, t_stat, p_valor, d = pipeline_testes_estatisticos(dados_limpos)
fig, ax1, ax2, ax3, ax4 = pipeline_visualizacoes_1_2(dados_limpos, controle, experimental)
pipeline_visualizacoes_3_4(dados_limpos, ax3, ax4)
dados_finais = pipeline_relatorio_final(dados_limpos, controle, experimental, t_stat, p_valor, d)
\end{lstlisting}
\end{examplebox}

\section{Dicas de Performance e Otimização}

Quando trabalhando com datasets grandes, performance se torna crucial.

\begin{warningbox}
\textbf{Armadilhas Comuns de Performance:}

\begin{enumerate}
    \item \textbf{Loops Python em dados grandes}: Use operações vetorizadas
    \item \textbf{Cópias desnecessárias}: Use views quando possível
    \item \textbf{Tipos de dados inadequados}: int64 vs int32 pode economizar 50\% da memória
    \item \textbf{Operações não otimizadas}: Use métodos nativos do pandas/numpy
    \item \textbf{Carregamento de dados ineficiente}: Use chunks para arquivos muito grandes
\end{enumerate}
\end{warningbox}

\begin{examplebox}
\textbf{Otimização de Performance - Comparações 1-2}

\begin{lstlisting}[language=Python]
import time

# Demonstração de diferenças de performance
def comparar_performance():
    """Compara diferentes abordagens para operações comuns"""
    
    # Dados de teste
    n = 1000000
    dados = pd.DataFrame({
        'grupo': np.random.choice(['A', 'B'], n),
        'valor': np.random.randn(n)
    })
    
    print("COMPARAÇÃO DE PERFORMANCE")
    print("="*40)
    
    # 1. Loop vs Vetorização
    print("1. Calcular quadrado dos valores")
    
    # Método lento: loop Python
    inicio = time.time()
    resultado_loop = []
    for valor in dados['valor']:
        resultado_loop.append(valor ** 2)
    tempo_loop = time.time() - inicio
    
    # Método rápido: vetorização NumPy
    inicio = time.time()
    resultado_numpy = dados['valor'] ** 2
    tempo_numpy = time.time() - inicio
    
    print(f"   Loop Python: {tempo_loop:.3f}s")
    print(f"   NumPy: {tempo_numpy:.3f}s")
    print(f"   Speedup: {tempo_loop/tempo_numpy:.1f}x")
    
    return dados, n
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Otimização de Performance - Comparação 2-3}

\begin{lstlisting}[language=Python]
def comparar_performance_continuacao(dados):
    # 2. Agrupamento eficiente
    print("\n2. Calcular média por grupo")
    
    # Método lento: loop manual
    inicio = time.time()
    resultado_manual = {}
    for grupo in ['A', 'B']:
        mask = dados['grupo'] == grupo
        resultado_manual[grupo] = dados.loc[mask, 'valor'].mean()
    tempo_manual = time.time() - inicio
    
    # Método rápido: groupby
    inicio = time.time()
    resultado_groupby = dados.groupby('grupo')['valor'].mean()
    tempo_groupby = time.time() - inicio
    
    print(f"   Loop manual: {tempo_manual:.3f}s")
    print(f"   GroupBy: {tempo_groupby:.3f}s")
    print(f"   Speedup: {tempo_manual/tempo_groupby:.1f}x")
    
    return resultado_manual, resultado_groupby
\end{lstlisting}
\end{examplebox}

\begin{examplebox}
\textbf{Otimização de Performance - Otimização de Memória}

\begin{lstlisting}[language=Python]
def otimizacao_memoria(n):
    # 3. Otimização de tipos
    print("\n3. Otimização de memória")
    
    # DataFrame com tipos não otimizados
    df_original = pd.DataFrame({
        'int_col': np.random.randint(0, 100, n),  # int64 por padrão
        'float_col': np.random.randn(n),  # float64 por padrão
        'cat_col': np.random.choice(['A', 'B', 'C'], n)  # object por padrão
    })
    
    # DataFrame com tipos otimizados
    df_otimizado = pd.DataFrame({
        'int_col': np.random.randint(0, 100, n).astype('int8'),  # Suficiente para 0-100
        'float_col': np.random.randn(n).astype('float32'),  # Precisão suficiente
        'cat_col': pd.Categorical(np.random.choice(['A', 'B', 'C'], n))  # Categórico
    })
    
    memoria_original = df_original.memory_usage(deep=True).sum() / 1024**2
    memoria_otimizada = df_otimizado.memory_usage(deep=True).sum() / 1024**2
    
    print(f"   Memória original: {memoria_original:.1f} MB")
    print(f"   Memória otimizada: {memoria_otimizada:.1f} MB")
    print(f"   Economia: {(1 - memoria_otimizada/memoria_original)*100:.1f}%")

# Executar comparação completa
dados, n = comparar_performance()
resultado_manual, resultado_groupby = comparar_performance_continuacao(dados)
otimizacao_memoria(n)
\end{lstlisting}
\end{examplebox}

\section{Exercícios Práticos}

Para dominar essas bibliotecas, pratique com estes exercícios baseados em cenários reais de pesquisa:

\begin{examplebox}
\textbf{Exercício 1: Análise de Dados Longitudinais}

Crie um dataset simulando medições mensais de 100 pacientes ao longo de 12 meses. Cada paciente tem: grupo (controle/tratamento), idade, e scores de ansiedade/depressão. Analise a evolução temporal por grupo e identifique fatores preditivos.

\textbf{Exercício 2: Análise de Questionário Complexo}

Simule dados de um questionário com 50 perguntas (escalas Likert 1-7) respondido por 500 pessoas. Realize análise fatorial exploratória para identificar dimensões latentes e crie visualizações das correlações entre fatores.

\textbf{Exercício 3: Dados de Experimento Psicofísico}

Simule dados de tempo de reação em diferentes condições experimentais (2 grupos × 3 condições × 100 trials por participante). Analise efeitos principais e interações, removendo outliers e criando visualizações apropriadas.
\end{examplebox}

Este capítulo estabeleceu o domínio das três bibliotecas fundamentais do ecossistema científico Python. No próximo capítulo, exploraremos métodos estatísticos avançados e como implementá-los usando essas ferramentas, incluindo testes de hipóteses robustos, análise de regressão e métodos de reamostragem.