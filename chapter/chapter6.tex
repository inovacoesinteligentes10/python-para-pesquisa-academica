% =============================================================================
% CAPÍTULO 6: COLETA E AQUISIÇÃO DE DADOS
% =============================================================================

\chapter{Coleta e Aquisição de Dados}

\lettrine{A}{era digital} transformou fundamentalmente a forma como pesquisadores coletam dados. Onde antes dependíamos de questionários impressos e coleta manual, hoje temos acesso a vastas fontes de dados online, APIs de plataformas digitais, sensores IoT e bases de dados públicas. Python oferece um arsenal completo de ferramentas para automatizar e sistematizar a coleta de dados, permitindo que pesquisadores acessem informações antes inimagináveis em escala e velocidade.

\section{Web Scraping para Pesquisa}

Web scraping é a técnica de extrair dados automaticamente de páginas web. Para pesquisadores, essa ferramenta abre possibilidades de coletar dados de redes sociais, sites de notícias, repositórios acadêmicos, bases governamentais e muito mais.

\subsection{Fundamentos do Web Scraping Ético}

Antes de começar a coletar dados da web, é crucial entender os aspectos éticos e legais envolvidos.

\begin{warningbox}
\textbf{Princípios Éticos do Web Scraping:}
\begin{itemize}
    \item \textbf{Respeite o robots.txt:} Sempre verifique as diretrizes de scraping do site
    \item \textbf{Limite a frequência:} Use delays entre requisições para não sobrecarregar servidores
    \item \textbf{Identifique-se:} Use User-Agent apropriado e contato para transparência
    \item \textbf{Dados pessoais:} Nunca colete informações pessoais identificáveis sem consentimento
    \item \textbf{Termos de uso:} Sempre leia e respeite os termos de serviço dos sites
\end{itemize}
\end{warningbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/data_collection/ethical_scraper.py
import requests
from bs4 import BeautifulSoup
import time
import random
from urllib.robotparser import RobotFileParser
import logging
from typing import List, Dict, Optional

class EthicalScraper:
    """Scraper que segue princípios éticos de coleta"""
    
    def __init__(self, user_agent="Research Bot", contact_email="researcher@university.edu"):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': f'{user_agent} ({contact_email})',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        self.logger = logging.getLogger(__name__)
        self.delay_range = (1, 3)  # Segundos entre requisições
        
    def can_fetch(self, url: str) -> bool:
        """Verifica se é permitido fazer scraping da URL"""
        try:
            rp = RobotFileParser()
            rp.set_url(f"{url.split('/')[0]}//{url.split('/')[2]}/robots.txt")
            rp.read()
            return rp.can_fetch(self.session.headers['User-Agent'], url)
        except:
            # Se não conseguir acessar robots.txt, assume que é permitido
            return True
    
    def respectful_request(self, url: str, **kwargs) -> Optional[requests.Response]:
        """Faz requisição respeitando delays e robots.txt"""
        if not self.can_fetch(url):
            self.logger.warning(f"Robots.txt proíbe acesso a {url}")
            return None

            \end{lstlisting}
\end{pythonbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
        # Delay aleatório entre requisições
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)
        
        try:
            response = self.session.get(url, **kwargs)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            self.logger.error(f"Erro ao acessar {url}: {e}")
            return None
\end{lstlisting}
\end{pythonbox}

\newpage

\begin{researchbox}
\textbf{Caso Real - Análise de Cobertura Midiática:}

Uma pesquisadora em comunicação social estuda a cobertura de mudanças climáticas em portais de notícias:

\begin{lstlisting}[language=Python]
# src/data_collection/news_scraper.py
from ethical_scraper import EthicalScraper
import pandas as pd
from datetime import datetime
import re
from urllib.parse import urljoin

class NewsArticleScraper(EthicalScraper):
    """Coletor especializado para artigos de notícias"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.articles = []
        
    def extract_article_data(self, soup, url: str) -> Dict:
        """Extrai dados estruturados de um artigo"""
        article_data = {
            'url': url,
            'collected_at': datetime.now(),
            'title': '',
            'content': '',
            'author': '',
            'publish_date': ''
        }
        
        # Título do artigo
        title_selectors = ['h1', '.article-title', '.post-title', 'title']
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                article_data['title'] = title_elem.get_text().strip()
                break
                
        # Conteúdo principal
        content_selectors = ['.article-content', '.post-content', 'article']
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # Remove scripts e estilos
                for script in content_elem(["script", "style"]):
                    script.decompose()
                article_data['content'] = content_elem.get_text().strip()
                break
        
        return article_data
\end{lstlisting}
\end{researchbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# Continuação: src/data_collection/news_scraper.py
    
    def search_climate_articles(self, base_urls: List[str], 
                               keywords: List[str], 
                               max_articles: int = 100) -> pd.DataFrame:
        """Busca artigos sobre mudanças climáticas"""
        self.logger.info(f"Iniciando coleta de até {max_articles} artigos")
        
        for base_url in base_urls:
            self.logger.info(f"Processando {base_url}")
            
            # Buscar URLs de artigos
            article_urls = self._find_article_urls(base_url)
            
            for url in article_urls[:max_articles]:
                response = self.respectful_request(url)
                if response:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    article_data = self.extract_article_data(soup, url)
                    
                    # Verificar se contém palavras-chave sobre clima
                    if self._contains_climate_keywords(article_data['content'], keywords):
                        self.articles.append(article_data)
                        self.logger.info(f"Artigo coletado: {article_data['title'][:50]}...")
                        
                if len(self.articles) >= max_articles:
                    break
                    
        return pd.DataFrame(self.articles)
    
    def _contains_climate_keywords(self, text: str, keywords: List[str]) -> bool:
        """Verifica se o texto contém palavras-chave relacionadas ao clima"""
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in keywords)
    
    def _find_article_urls(self, base_url: str) -> List[str]:
        """Encontra URLs de artigos no site"""
        response = self.respectful_request(base_url)
        if not response:
            return []
\end{lstlisting}
\end{pythonbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]            
        soup = BeautifulSoup(response.content, 'html.parser')
        links = soup.find_all('a', href=True)
        
        article_urls = []
        for link in links:
            href = link['href']
            if href.startswith('/'):
                href = urljoin(base_url, href)
            
            # Filtrar apenas URLs que parecem ser artigos
            if '/article/' in href or '/news/' in href:
                article_urls.append(href)
                
        return list(set(article_urls))  # Remove duplicatas
\end{lstlisting}
\end{pythonbox}

\section{APIs e Acesso a Bancos de Dados Públicos}

APIs (Application Programming Interfaces) oferecem uma forma estruturada e eficiente de acessar dados. Muitas instituições governamentais, organizações internacionais e plataformas digitais disponibilizam APIs públicas para pesquisadores.

\subsection{Cliente Genérico para APIs REST}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/data_collection/api_client.py
import requests
import pandas as pd
from typing import Dict, List, Optional
import time
from datetime import datetime

class APIClient:
    """Cliente genérico para APIs REST"""
    
    def __init__(self, base_url: str, api_key: Optional[str] = None, 
                 rate_limit: float = 1.0):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.rate_limit = rate_limit
        self.session = requests.Session()
        
        # Headers padrão
        self.session.headers.update({
            'User-Agent': 'Research-Python-Client/1.0',
            'Accept': 'application/json',
        })
        
        if api_key:
            self.session.headers.update({
                'Authorization': f'Bearer {api_key}'
            })
            
        self.last_request_time = 0
        
    def _respect_rate_limit(self):
        """Respeita limite de taxa da API"""
        now = time.time()
        time_since_last = now - self.last_request_time
        
        if time_since_last < self.rate_limit:
            time.sleep(self.rate_limit - time_since_last)
            
        self.last_request_time = time.time()
\end{lstlisting}
\end{pythonbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]        
    def get(self, endpoint: str, params: Dict = None) -> Dict:
        """Faz requisição GET para a API"""
        self._respect_rate_limit()
        
        url = f"{self.base_url}/{endpoint.lstrip('/')}"
        
        try:
            response = self.session.get(url, params=params)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f"Erro na requisição para {url}: {e}")
            return {}
\end{lstlisting}
\end{pythonbox}

\newpage

\begin{researchbox}
\textbf{Caso Real - Dados Econômicos do Banco Mundial:}

Um economista estuda indicadores de desenvolvimento econômico usando a API do Banco Mundial:

\begin{lstlisting}[language=Python]
# src/data_collection/world_bank_api.py
from api_client import APIClient
import pandas as pd
from typing import List

class WorldBankAPI(APIClient):
    """Cliente especializado para API do Banco Mundial"""
    
    def __init__(self):
        super().__init__("http://api.worldbank.org/v2", rate_limit=0.5)
        
    def get_indicators(self, country_codes: List[str], 
                      indicator_codes: List[str], 
                      start_year: int = 2000, 
                      end_year: int = 2023) -> pd.DataFrame:
        """Obtém indicadores econômicos para países específicos"""
        all_data = []
        
        for country in country_codes:
            for indicator in indicator_codes:
                endpoint = f"countries/{country}/indicators/{indicator}"
                params = {
                    'format': 'json',
                    'date': f'{start_year}:{end_year}',
                    'per_page': 100
                }
                
                data = self.get(endpoint, params)
                
                if len(data) > 1 and data[1]:
                    for record in data[1]:
                        if record['value'] is not None:
                            all_data.append({
                                'country_code': country,
                                'indicator_code': indicator,
                                'year': int(record['date']),
                                'value': float(record['value']),
                                'country_name': record['country']['value'],
                                'indicator_name': record['indicator']['value']
                            })
        
        return pd.DataFrame(all_data)

\end{lstlisting}
\end{researchbox}

\begin{researchbox}
\begin{lstlisting}
# Exemplo de uso
wb_api = WorldBankAPI()

# Países de interesse
countries = ['BRA', 'USA', 'CHN', 'DEU', 'JPN']

# Indicadores econômicos
indicators = [
    'NY.GDP.PCAP.CD',    # PIB per capita
    'SP.POP.TOTL',       # População total
    'SL.UEM.TOTL.ZS',    # Taxa de desemprego
]

# Coletar dados
economic_data = wb_api.get_indicators(countries, indicators, 2010, 2023)
economic_data.to_csv('data/raw/world_bank_indicators.csv', index=False)
\end{lstlisting}
\end{researchbox}

\section{Integração com Redes Sociais}

Plataformas de mídia social são fontes ricas de dados para pesquisa em ciências sociais, comunicação, psicologia e marketing.

\subsection{Coleta de Dados do Twitter/X}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/data_collection/twitter_collector.py
import tweepy
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict

class TwitterDataCollector:
    """Coletor de dados do Twitter usando API v2"""
    
    def __init__(self, bearer_token: str):
        self.client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)
        
    def search_tweets(self, query: str, max_results: int = 100, 
                     days_back: int = 7) -> pd.DataFrame:
        """Busca tweets recentes baseado em query"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=days_back)
        
        tweets_data = []
        
        # Configurar campos para coletar
        tweet_fields = ['created_at', 'author_id', 'public_metrics', 'lang']
        
        try:
            tweets = tweepy.Paginator(
                self.client.search_recent_tweets,
                query=query,
                tweet_fields=tweet_fields,
                start_time=start_time,
                end_time=end_time,
                max_results=min(100, max_results)
            ).flatten(limit=max_results)
            
            for tweet in tweets:
                tweet_data = {
                    'id': tweet.id,
                    'text': tweet.text,
                    'created_at': tweet.created_at,
                    'author_id': tweet.author_id,
                    'retweet_count': tweet.public_metrics['retweet_count'],
                    'like_count': tweet.public_metrics['like_count'],
                    'lang': tweet.lang if hasattr(tweet, 'lang') else None
                }
                tweets_data.append(tweet_data)
\end{lstlisting}
\end{pythonbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
                
        except Exception as e:
            print(f"Erro ao coletar tweets: {e}")
            
        return pd.DataFrame(tweets_data)
    
    def analyze_sentiment_trends(self, tweets_df: pd.DataFrame) -> pd.DataFrame:
        """Analisa tendências de sentimento nos tweets coletados"""
        if tweets_df.empty:
            return pd.DataFrame()
        
        from textblob import TextBlob
        
        # Análise de sentimento
        tweets_df['sentiment'] = tweets_df['text'].apply(
            lambda x: TextBlob(x).sentiment.polarity
        )
        
        # Agregar por hora
        tweets_df['hour'] = tweets_df['created_at'].dt.floor('H')
        hourly_sentiment = tweets_df.groupby('hour').agg({
            'sentiment': 'mean',
            'like_count': 'mean',
            'id': 'count'
        }).rename(columns={'id': 'tweet_count'})
        
        return hourly_sentiment
\end{lstlisting}
\end{pythonbox}

\section{Monitoramento Contínuo}

Para estudos longitudinais, é essencial implementar sistemas de coleta contínua.

\subsection{Agendamento de Coletas}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/data_collection/scheduled_collector.py
import schedule
import time
import logging
from datetime import datetime
from typing import Callable, Dict

class ScheduledDataCollector:
    """Sistema para coleta automatizada e agendada de dados"""
    
    def __init__(self, base_config: Dict):
        self.config = base_config
        self.collectors = {}
        self.logger = logging.getLogger(__name__)
        
    def register_collector(self, name: str, collector_func: Callable, 
                          schedule_config: Dict):
        """Registra um coletor com sua configuração de agendamento"""
        self.collectors[name] = {
            'function': collector_func,
            'schedule': schedule_config,
            'last_run': None,
            'total_runs': 0
        }
        
        # Configurar agendamento
        if schedule_config['type'] == 'interval':
            if schedule_config['unit'] == 'hours':
                schedule.every(schedule_config['value']).hours.do(
                    self._run_collector, name, collector_func
                )
            elif schedule_config['unit'] == 'days':
                schedule.every(schedule_config['value']).days.do(
                    self._run_collector, name, collector_func
                )
                
    def _run_collector(self, name: str, func: Callable):
        """Executa um coletor específico"""
        try:
            self.logger.info(f"Iniciando coleta: {name}")
            result = func()
            
            self.collectors[name]['last_run'] = datetime.now()
            self.collectors[name]['total_runs'] += 1
\end{lstlisting}
\end{pythonbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
            
            # Salvar dados se especificado
            if 'output_path' in self.config:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = f"{self.config['output_path']}/{name}_{timestamp}.csv"
                if hasattr(result, 'to_csv'):
                    result.to_csv(output_file, index=False)
                    
            self.logger.info(f"Coleta {name} concluída")
            
        except Exception as e:
            self.logger.error(f"Erro na coleta {name}: {e}")
    
    def start_monitoring(self):
        """Inicia monitoramento contínuo"""
        self.logger.info("Iniciando sistema de coleta automatizada")
        while True:
            schedule.run_pending()
            time.sleep(60)
\end{lstlisting}
\end{pythonbox}

\section{Validação e Qualidade}

A coleta automatizada exige sistemas robustos de validação.

\subsection{Framework de Validação}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/data_collection/data_validator.py
import pandas as pd
from datetime import datetime
from typing import Dict, Callable

class DataQualityValidator:
    """Sistema de validação de qualidade de dados coletados"""
    
    def __init__(self):
        self.validation_rules = {}
        
    def add_validation_rule(self, rule_name: str, validator_func: Callable):
        """Adiciona regra de validação personalizada"""
        self.validation_rules[rule_name] = validator_func
        
    def validate_dataset(self, data: pd.DataFrame, dataset_name: str = "unnamed") -> Dict:
        """Executa todas as validações em um dataset"""
        report = {
            'dataset_name': dataset_name,
            'timestamp': datetime.now(),
            'total_rows': len(data),
            'total_columns': len(data.columns),
            'issues': [],
            'quality_score': 100.0
        }
        
        # Validações básicas
        missing_data = data.isnull().sum()
        duplicates = data.duplicated().sum()
        
        # Penalizar qualidade por problemas
        missing_penalty = (missing_data.sum() / (len(data) * len(data.columns))) * 50
        duplicate_penalty = (duplicates / len(data)) * 30
        
        report['quality_score'] -= (missing_penalty + duplicate_penalty)

\end{lstlisting}
\end{pythonbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]        
        # Executar validações customizadas
        for rule_name, rule_func in self.validation_rules.items():
            try:
                result = rule_func(data)
                if not result['passed']:
                    report['issues'].append({
                        'rule': rule_name,
                        'message': result['message']
                    })
                    report['quality_score'] -= 10
            except Exception as e:
                report['issues'].append({
                    'rule': rule_name,
                    'message': f"Erro na validação: {str(e)}"
                })
        
        return report

# Exemplo de uso
validator = DataQualityValidator()

def validate_date_range(data):
    """Valida se datas estão em range aceitável"""
    if 'created_at' not in data.columns:
        return {'passed': True, 'message': 'Sem coluna de data'}
    
    dates = pd.to_datetime(data['created_at'], errors='coerce')
    future_dates = (dates > datetime.now()).sum()
    
    if future_dates > 0:
        return {'passed': False, 'message': f'{future_dates} datas futuras encontradas'}
    
    return {'passed': True, 'message': 'Datas válidas'}

validator.add_validation_rule('date_range', validate_date_range)
\end{lstlisting}
\end{pythonbox}

\section{Considerações Éticas e Legais}

\subsection{Framework Ético para Coleta}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/data_collection/ethical_framework.py
import hashlib
import pandas as pd
import re
from typing import List

class EthicalDataProcessor:
    """Processador que aplica princípios éticos na coleta de dados"""
    
    def __init__(self, research_purpose: str, institution: str):
        self.research_purpose = research_purpose
        self.institution = institution
        
    def anonymize_identifiers(self, data: pd.DataFrame, 
                             identifier_columns: List[str]) -> pd.DataFrame:
        """Remove ou anonimiza identificadores pessoais"""
        anonymized_data = data.copy()
        
        for col in identifier_columns:
            if col in anonymized_data.columns:
                # Criar hash irreversível para IDs únicos
                anonymized_data[f"{col}_hash"] = anonymized_data[col].apply(
                    lambda x: hashlib.sha256(str(x).encode()).hexdigest()[:12]
                )
                # Remover coluna original
                anonymized_data = anonymized_data.drop(columns=[col])
                
        return anonymized_data
    
    def remove_sensitive_content(self, text_series: pd.Series) -> pd.Series:
        """Remove conteúdo potencialmente sensível de textos"""
        cleaned_text = text_series.copy()
        
        # Padrões para remoção
        patterns = {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone': r'\b\d{2,3}[-.\s]?\d{4,5}[-.\s]?\d{4}\b',
            'cpf': r'\b\d{3}\.?\d{3}\.?\d{3}[-.]?\d{2}\b'
        }
        
        for pattern_type, pattern in patterns.items():
            cleaned_text = cleaned_text.str.replace(
                pattern, f'[{pattern_type.upper()}_REMOVED]', regex=True
            )
            
        return cleaned_text
    \end{lstlisting}
\end{pythonbox}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
    def generate_privacy_report(self, processed_data: pd.DataFrame) -> Dict:
        """Gera relatório de privacidade"""
        return {
            'timestamp': datetime.now(),
            'processed_rows': len(processed_data),
            'research_purpose': self.research_purpose,
            'institution': self.institution,
            'privacy_measures': [
                'Identificadores removidos',
                'Conteúdo sensível filtrado',
                'Dados anonimizados'
            ]
        }

# Exemplo de uso
processor = EthicalDataProcessor(
    research_purpose="Análise de sentimento político",
    institution="Universidade XYZ"
)

# Processar dados de forma ética
sample_data = pd.DataFrame({
    'user_id': ['user123', 'user456'],
    'text': ['Meu email é joao@email.com', 'Política é importante'],
    'timestamp': pd.date_range('2024-01-01', periods=2)
})

anonymized_data = processor.anonymize_identifiers(sample_data, ['user_id'])
anonymized_data['text'] = processor.remove_sensitive_content(anonymized_data['text'])

print("Dados processados de forma ética:")
print(anonymized_data)
\end{lstlisting}
\end{pythonbox}

\section{Conclusão do Capítulo}

A coleta e aquisição de dados representa uma das fases mais críticas da pesquisa moderna. Python oferece um ecossistema rico de ferramentas que permitem aos pesquisadores acessar dados de múltiplas fontes de forma eficiente, ética e sistematizada.

Os elementos essenciais abordados neste capítulo incluem:

\textbf{Web Scraping Ético:} Técnicas para extrair dados da web respeitando diretrizes técnicas e éticas fundamentais.

\textbf{APIs e Integrações:} Acesso estruturado a dados através de interfaces programáticas, incluindo autenticação e gestão de limites.

\textbf{Redes Sociais:} Métodos especializados para coletar dados de plataformas digitais com análise de sentimento.

\textbf{Monitoramento Contínuo:} Sistemas automatizados para coleta longitudinal, essenciais para estudos dinâmicos.

\textbf{Validação de Qualidade:} Frameworks robustos para garantir integridade e confiabilidade dos dados.

\textbf{Ética e Privacidade:} Princípios e práticas para coleta responsável com proteção de dados pessoais.

\begin{examplebox}
\textbf{Principais Competências Desenvolvidas:}
\begin{itemize}
    \item Implementação de scrapers éticos e eficientes
    \item Integração com APIs públicas e privadas
    \item Coleta automatizada e agendada de dados
    \item Análise de redes sociais e padrões de interação
    \item Validação automática de qualidade de dados
    \item Aplicação de princípios éticos na coleta
    \item Processamento e anonimização de dados sensíveis
\end{itemize}
\end{examplebox}

No próximo capítulo, exploraremos como processar, limpar e preparar os dados coletados para análise, aplicando técnicas avançadas de processamento de linguagem natural que transformam dados textuais brutos em informações estruturadas prontas para investigação científica.