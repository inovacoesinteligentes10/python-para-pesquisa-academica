% =============================================================================
% CAPÍTULO 7: PROCESSAMENTO DE LINGUAGEM NATURAL (NLP)
% =============================================================================

\chapter{Processamento de Linguagem Natural (NLP)}

\lettrine{A}{linguagem humana} é uma das fontes mais ricas de dados para pesquisa acadêmica. Textos de redes sociais, entrevistas transcritas, documentos históricos, artigos científicos e corpus literários contêm insights valiosos sobre comportamento humano, tendências sociais, desenvolvimento de ideias e muito mais. Python oferece um ecossistema robusto de bibliotecas de NLP que permitem aos pesquisadores extrair significado, padrões e estruturas de dados textuais de forma sistemática e escalável.

\section{Fundamentos do Processamento de Texto}

Antes de realizar análises sofisticadas, é essencial dominar as técnicas fundamentais de preprocessamento e limpeza de texto. Estas etapas determinam a qualidade de todas as análises subsequentes.

\subsection{Limpeza e Normalização de Texto}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/nlp/text_preprocessor.py
import re
import string
import unicodedata
from typing import List, Dict, Optional
import pandas as pd

class TextPreprocessor:
    """Processador para limpeza e normalização de texto"""
    
    def __init__(self, language='portuguese'):
        self.language = language
        self.stopwords = self._load_stopwords()
        self.contractions = self._load_contractions()
        
    def _load_stopwords(self) -> set:
        """Carrega stopwords para o idioma especificado"""
        try:
            import nltk
            nltk.download('stopwords', quiet=True)
            from nltk.corpus import stopwords
            
            if self.language == 'portuguese':
                return set(stopwords.words('portuguese'))
            elif self.language == 'english':
                return set(stopwords.words('english'))
            else:
                return set(stopwords.words('english'))  # fallback
                
        except ImportError:
            # Stopwords básicas se NLTK não estiver disponível
            if self.language == 'portuguese':
                return {'de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 
                       'é', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais'}
            else:
                return {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
                       'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}
    
    def _load_contractions(self) -> Dict[str, str]:
        """Carrega mapeamento de contrações"""
        if self.language == 'portuguese':
            return {
                'não': 'não', 'nao': 'não', 'voce': 'você', 'vc': 'você',
                'pq': 'porque', 'pra': 'para', 'pro': 'para o'
            }
\end{lstlisting}
\end{pythonbox}
\begin{pythonbox}
\begin{lstlisting}[language=Python]            
            
        else:  # English
            return {
                "can't": "cannot", "won't": "will not", "n't": " not",
                "'re": " are", "'ve": " have", "'ll": " will", "'d": " would",
                "'m": " am", "it's": "it is", "that's": "that is"
            }
\end{lstlisting}
\end{pythonbox}

\newpage

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# Continuação: src/nlp/text_preprocessor.py
    
    def clean_text(self, text: str, 
                   remove_urls: bool = True,
                   remove_mentions: bool = True,
                   remove_hashtags: bool = False,
                   remove_numbers: bool = False,
                   remove_punctuation: bool = True,
                   lowercase: bool = True) -> str:
        """Aplica limpeza básica ao texto"""
        
        if not isinstance(text, str):
            return ""
            
        # Normalizar unicode
        text = unicodedata.normalize('NFKD', text)
        
        # Remover URLs
        if remove_urls:
            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # Remover menções (@usuario)
        if remove_mentions:
            text = re.sub(r'@\w+', '', text)
            
        # Remover hashtags (mas manter o texto)
        if remove_hashtags:
            text = re.sub(r'#(\w+)', r'\1', text)
            
        # Remover números
        if remove_numbers:
            text = re.sub(r'\d+', '', text)
            
        # Expandir contrações
        for contraction, expansion in self.contractions.items():
            text = re.sub(r'\b' + contraction + r'\b', expansion, text, flags=re.IGNORECASE)
            
        # Converter para minúsculas
        if lowercase:
            text = text.lower()
            
        # Remover pontuação
        if remove_punctuation:
            text = text.translate(str.maketrans('', '', string.punctuation))
            
        # Remover espaços extras
        text = re.sub(r'\s+', ' ', text).strip()
            
        return text
        \end{lstlisting}
\end{pythonbox}
\begin{pythonbox}
\begin{lstlisting}[language=Python]   
    def tokenize(self, text: str, remove_stopwords: bool = True) -> List[str]:
        """Tokeniza texto em palavras"""
        # Limpeza básica
        cleaned_text = self.clean_text(text)
        
        # Tokenização simples por espaços
        tokens = cleaned_text.split()
        
        # Remover stopwords
        if remove_stopwords:
            tokens = [token for token in tokens if token not in self.stopwords]
            
        # Filtrar tokens muito curtos
        tokens = [token for token in tokens if len(token) > 2]
        
        return tokens
\end{lstlisting}
\end{pythonbox}

\begin{researchbox}
\textbf{Caso Real - Análise de Comentários de Políticas Públicas:}

Uma pesquisadora em políticas públicas analisa comentários de consultas públicas governamentais:

\begin{lstlisting}[language=Python]
# src/nlp/public_consultation_analyzer.py
from text_preprocessor import TextPreprocessor
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

class PublicConsultationAnalyzer:
    """Analisador para comentários de consultas públicas"""
    
    def __init__(self):
        self.preprocessor = TextPreprocessor(language='portuguese')
        self.comments_df = None
        
    def load_and_clean_comments(self, filepath: str) -> pd.DataFrame:
        """Carrega e limpa comentários de consulta pública"""
        # Carregar dados
        self.comments_df = pd.read_csv(filepath)
        
        # Limpar textos
        self.comments_df['comment_cleaned'] = self.comments_df['comment'].apply(
            lambda x: self.preprocessor.clean_text(x, remove_hashtags=False)
        )
        
        # Tokenizar
        self.comments_df['tokens'] = self.comments_df['comment_cleaned'].apply(
            lambda x: self.preprocessor.tokenize(x)
        )
        
        # Filtrar comentários muito curtos
        self.comments_df = self.comments_df[
            self.comments_df['tokens'].apply(len) >= 3
        ]
        
        return self.comments_df
    
    def extract_key_themes(self, min_frequency: int = 5) -> Dict[str, int]:
        """Extrai temas principais dos comentários"""
        # Combinar todos os tokens
        all_tokens = []
        for tokens in self.comments_df['tokens']:
            all_tokens.extend(tokens)
\end{lstlisting}
\end{researchbox}
\begin{researchbox}
\begin{lstlisting}[language=Python]         
        # Contar frequências
        token_freq = Counter(all_tokens)
        
        # Filtrar por frequência mínima
        key_themes = {token: freq for token, freq in token_freq.items() 
                     if freq >= min_frequency}
        
        return dict(sorted(key_themes.items(), key=lambda x: x[1], reverse=True))
\end{lstlisting}
\end{researchbox}

\section{Análise de Sentimento}

A análise de sentimento é uma das aplicações mais comuns de NLP em pesquisa, permitindo quantificar atitudes, opiniões e emoções expressas em texto.

\subsection{Abordagens Básicas para Análise de Sentimento}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/nlp/sentiment_analyzer.py
import pandas as pd
from typing import Dict, List, Union
import numpy as np

class SentimentAnalyzer:
    """Analisador de sentimento com múltiplas abordagens"""
    
    def __init__(self, language='portuguese'):
        self.language = language
        self.lexicon = self._load_sentiment_lexicon()
        
    def _load_sentiment_lexicon(self) -> Dict[str, float]:
        """Carrega léxico de sentimento para o idioma"""
        if self.language == 'portuguese':
            # Léxico básico em português
            positive_words = {
                'bom': 1.0, 'ótimo': 2.0, 'excelente': 2.0, 'maravilhoso': 2.0,
                'legal': 1.0, 'bacana': 1.0, 'perfeito': 2.0, 'incrível': 2.0,
                'positivo': 1.0, 'feliz': 1.5, 'alegre': 1.5, 'satisfeito': 1.0,
                'aprovado': 1.0, 'concordo': 1.0, 'apoio': 1.5, 'gosto': 1.0
            }
            
            negative_words = {
                'ruim': -1.0, 'péssimo': -2.0, 'terrível': -2.0, 'horrível': -2.0,
                'negativo': -1.0, 'triste': -1.5, 'raiva': -1.5, 'ódio': -2.0,
                'contra': -1.0, 'discordo': -1.0, 'odeio': -2.0, 'detesto': -2.0,
                'reprovado': -1.0, 'rejeitado': -1.5, 'problema': -1.0
            }
            
            lexicon = {**positive_words, **negative_words}
            
        else:  # English fallback
            lexicon = {
                'good': 1.0, 'great': 2.0, 'excellent': 2.0, 'amazing': 2.0,
                'bad': -1.0, 'terrible': -2.0, 'awful': -2.0, 'horrible': -2.0,
                'love': 2.0, 'like': 1.0, 'hate': -2.0, 'dislike': -1.0
            }
            
        return lexicon
    \end{lstlisting}
\end{pythonbox}
\begin{pythonbox}
\begin{lstlisting}[language=Python]   
    def lexicon_based_sentiment(self, text: str) -> Dict[str, Union[float, str]]:
        """Análise de sentimento baseada em léxico"""
        # Preprocessar texto
        words = text.lower().split()
        
        # Calcular score
        scores = [self.lexicon.get(word, 0) for word in words]
        
        if not scores:
            return {'score': 0.0, 'label': 'neutral', 'confidence': 0.0}
            
        avg_score = np.mean(scores)
        
        # Classificar sentimento
        if avg_score > 0.1:
            label = 'positive'
        elif avg_score < -0.1:
            label = 'negative'
        else:
            label = 'neutral'
            
        # Confiança baseada na magnitude do score
        confidence = min(abs(avg_score), 1.0)
        
        return {
            'score': round(avg_score, 3),
            'label': label,
            'confidence': round(confidence, 3)
        }
\end{lstlisting}
\end{pythonbox}

\newpage

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# Continuação: src/nlp/sentiment_analyzer.py
    
    def analyze_sentiment_dataset(self, texts: List[str], 
                                 method: str = 'lexicon') -> pd.DataFrame:
        """Analisa sentimento de um dataset de textos"""
        results = []
        
        for text in texts:
            if method == 'lexicon':
                sentiment = self.lexicon_based_sentiment(text)
            else:
                sentiment = self.lexicon_based_sentiment(text)
                
            results.append({
                'text': text,
                **sentiment
            })
        
        return pd.DataFrame(results)
    
    def sentiment_trends_over_time(self, df: pd.DataFrame, 
                                  text_col: str, 
                                  date_col: str,
                                  freq: str = 'D') -> pd.DataFrame:
        """Analisa tendências de sentimento ao longo do tempo"""
        # Analisar sentimentos
        sentiments = self.analyze_sentiment_dataset(df[text_col].tolist())
        
        # Adicionar datas
        sentiments['date'] = pd.to_datetime(df[date_col])
        
        # Agrupar por período
        trends = sentiments.set_index('date').resample(freq).agg({
            'score': 'mean',
            'label': lambda x: x.value_counts().index[0] if len(x) > 0 else 'neutral'
        })
        
        return trends

# Exemplo de uso
analyzer = SentimentAnalyzer(language='portuguese')

# Textos exemplo
sample_texts = [
    "Esta política pública é excelente e vai ajudar muito!",
    "Discordo totalmente desta proposta, é muito ruim",
    "A medida tem pontos positivos e negativos",
    "Não entendi direito a proposta"
]

    \end{lstlisting}
\end{pythonbox}
\begin{pythonbox}
\begin{lstlisting}[language=Python]   

# Analisar sentimentos
results = analyzer.analyze_sentiment_dataset(sample_texts)
print("Análise de Sentimento:")
print(results[['text', 'score', 'label']].head())
\end{lstlisting}
\end{pythonbox}

\section{Extração de Entidades e Modelagem de Tópicos}

A extração de informações estruturadas de texto não estruturado é fundamental para muitas análises de pesquisa.

\subsection{Reconhecimento de Entidades Nomeadas (NER)}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/nlp/entity_extractor.py
import re
import pandas as pd
from typing import List, Dict
from collections import defaultdict

class EntityExtractor:
    """Extrator de entidades nomeadas de texto"""
    
    def __init__(self, language='portuguese'):
        self.language = language
        self.patterns = self._load_patterns()
        
    def _load_patterns(self) -> Dict[str, str]:
        """Carrega padrões regex para diferentes tipos de entidades"""
        return {
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'phone_br': r'\b(?:\(\d{2}\)\s?)?\d{4,5}[-\s]?\d{4}\b',
            'cpf': r'\b\d{3}\.?\d{3}\.?\d{3}[-.]?\d{2}\b',
            'date_br': r'\b\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4}\b',
            'money_br': r'R\$\s*\d{1,3}(?:\.\d{3})*(?:,\d{2})?',
            'hashtag': r'#\w+',
            'mention': r'@\w+',
        }
        \end{lstlisting}
\end{pythonbox}
\begin{pythonbox}
\begin{lstlisting}[language=Python]   
    def extract_entities(self, text: str, 
                        entity_types: List[str] = None) -> Dict[str, List[str]]:
        """Extrai entidades específicas do texto"""
        if entity_types is None:
            entity_types = list(self.patterns.keys())
            
        entities = defaultdict(list)
        
        for entity_type in entity_types:
            if entity_type in self.patterns:
                pattern = self.patterns[entity_type]
                matches = re.findall(pattern, text, re.IGNORECASE)
                entities[entity_type].extend(matches)
                
        # Remover duplicatas mantendo ordem
        for entity_type in entities:
            entities[entity_type] = list(dict.fromkeys(entities[entity_type]))
            
        return dict(entities)
    
    def extract_named_entities_spacy(self, text: str) -> Dict[str, List[Dict]]:
        """Extrai entidades usando spaCy (se disponível)"""
        try:
            import spacy
            
            # Tentar carregar modelo em português
            try:
                nlp = spacy.load("pt_core_news_sm")
            except OSError:
                return self._fallback_entity_extraction(text)
            
            doc = nlp(text)
            entities = defaultdict(list)
            
            for ent in doc.ents:
                entities[ent.label_].append({
                    'text': ent.text,
                    'start': ent.start_char,
                    'end': ent.end_char
                })
                
            return dict(entities)
            
        except ImportError:
            return self._fallback_entity_extraction(text)
\end{lstlisting}
\end{pythonbox}

\newpage

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# Continuação: src/nlp/entity_extractor.py
    
    def _fallback_entity_extraction(self, text: str) -> Dict[str, List[Dict]]:
        """Extração básica quando spaCy não está disponível"""
        basic_patterns = {
            'PERSON': r'\b[A-ZÁÉÍÓÚÃÕÇ][a-záéíóúãõç]+(?:\s+[A-ZÁÉÍÓÚÃÕÇ][a-záéíóúãõç]+)+\b',
            'ORG': r'\b(?:Empresa|Instituto|Universidade)\s+[A-ZÁÉÍÓÚÃÕÇ][a-záéíóúãõç\s]+\b',
            'LOC': r'\b(?:São Paulo|Rio de Janeiro|Brasil|Brasília)\b',
        }
        
        entities = defaultdict(list)
        
        for entity_type, pattern in basic_patterns.items():
            matches = re.finditer(pattern, text)
            for match in matches:
                entities[entity_type].append({
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end()
                })
                
        return dict(entities)

# Exemplo de uso
extractor = EntityExtractor(language='portuguese')

sample_text = """
João Silva trabalhou na Empresa ABC de São Paulo.
Seu email é joao@empresa.com e telefone (11) 98765-4321.
A empresa investiu R$ 1.500.000,00 em tecnologia.
"""

# Extrair entidades
basic_entities = extractor.extract_entities(sample_text)
named_entities = extractor.extract_named_entities_spacy(sample_text)

print("Entidades encontradas:")
for entity_type, entities in basic_entities.items():
    if entities:
        print(f"{entity_type}: {entities}")
\end{lstlisting}
\end{pythonbox}

\subsection{Modelagem de Tópicos}

\begin{pythonbox}
\begin{lstlisting}[language=Python]
# src/nlp/topic_modeling.py
import pandas as pd
import numpy as np
from typing import List, Dict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

class TopicModeler:
    """Modelador de tópicos para análise de grandes corpus"""
    
    def __init__(self, language='portuguese'):
        self.language = language
        self.vectorizer = None
        self.lda_model = None
        self.feature_names = None
        
    def prepare_documents(self, documents: List[str], 
                         min_df: int = 2, 
                         max_df: float = 0.8,
                         max_features: int = 1000) -> np.ndarray:
        """Prepara documentos para modelagem de tópicos"""
        if self.language == 'portuguese':
            stop_words = ['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 
                         'é', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais']
        else:
            stop_words = 'english'
        
        self.vectorizer = TfidfVectorizer(
            max_df=max_df,
            min_df=min_df,
            max_features=max_features,
            stop_words=stop_words,
            lowercase=True
        )
        
        doc_term_matrix = self.vectorizer.fit_transform(documents)
        self.feature_names = self.vectorizer.get_feature_names_out()
        
        return doc_term_matrix
        \end{lstlisting}
\end{pythonbox}
\begin{pythonbox}
\begin{lstlisting}[language=Python]   
    def fit_lda_model(self, doc_term_matrix: np.ndarray, 
                     n_topics: int = 5) -> LatentDirichletAllocation:
        """Treina modelo LDA para extração de tópicos"""
        self.lda_model = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            max_iter=100
        )
        
        self.lda_model.fit(doc_term_matrix)
        return self.lda_model
    
    def get_topic_words(self, n_words: int = 10) -> List[List[str]]:
        """Extrai palavras mais importantes para cada tópico"""
        if not self.lda_model or self.feature_names is None:
            return []
            
        topics = []
        for topic_idx, topic in enumerate(self.lda_model.components_):
            top_words_idx = topic.argsort()[-n_words:][::-1]
            top_words = [self.feature_names[i] for i in top_words_idx]
            topics.append(top_words)
            
        return topics
    
    def analyze_corpus_topics(self, documents: List[str], 
                             n_topics: int = 5) -> Dict:
        """Análise completa de tópicos em um corpus"""
        doc_term_matrix = self.prepare_documents(documents)
        self.fit_lda_model(doc_term_matrix, n_topics)
        topic_words = self.get_topic_words()
        
        return {
            'topic_words': topic_words,
            'n_topics': n_topics,
            'vocab_size': len(self.feature_names)
        }

# Exemplo de uso
modeler = TopicModeler(language='portuguese')

sample_documents = [
    "A política de saúde pública precisa de mais investimento",
    "Educação é fundamental para o desenvolvimento do país",
    "O transporte público está em crise",
    "Segurança pública é prioridade",
]
    \end{lstlisting}
\end{pythonbox}
\begin{pythonbox}
\begin{lstlisting}[language=Python]   
results = modeler.analyze_corpus_topics(sample_documents, n_topics=2)

print("Tópicos encontrados:")
for i, words in enumerate(results['topic_words']):
    print(f"Tópico {i}: {', '.join(words[:5])}")
\end{lstlisting}
\end{pythonbox}

\section{Conclusão do Capítulo}

O processamento de linguagem natural representa uma das áreas mais dinâmicas e impactantes da ciência de dados aplicada à pesquisa acadêmica. Python oferece um ecossistema abrangente que permite aos pesquisadores extrair insights profundos de dados textuais, desde análises básicas até técnicas sofisticadas de modelagem.

Os elementos fundamentais abordados neste capítulo incluem:

\textbf{Preprocessamento Robusto:} Técnicas essenciais de limpeza, normalização e tokenização que formam a base de todas as análises subsequentes.

\textbf{Análise de Sentimento:} Métodos para quantificar atitudes e emoções expressas em texto, fundamentais para estudos de opinião pública e comportamento social.

\textbf{Extração de Entidades:} Identificação automática de pessoas, organizações, locais e conceitos importantes em grandes volumes de texto.

\textbf{Modelagem de Tópicos:} Descoberta automática de temas latentes em corpus extensos, revelando estruturas temáticas ocultas.

\begin{examplebox}
\textbf{Principais Competências Desenvolvidas:}
\begin{itemize}
    \item Implementação de pipelines robustos de preprocessamento
    \item Análise de sentimento com múltiplas abordagens
    \item Extração automática de entidades nomeadas
    \item Modelagem de tópicos com LDA
    \item Processamento de texto em português e inglês
    \item Integração de técnicas de NLP em fluxos de pesquisa
    \item Visualização de resultados de análise textual
\end{itemize}
\end{examplebox}

As técnicas apresentadas neste capítulo transformam texto não estruturado em dados quantitativos analisáveis, abrindo novas possibilidades para pesquisa em humanidades digitais, ciências sociais computacionais e estudos interdisciplinares que dependem de análise textual em larga escala.

No próximo capítulo, exploraremos técnicas de machine learning aplicadas à pesquisa acadêmica, construindo sobre os fundamentos de NLP para desenvolver modelos preditivos e classificadores que podem automatizar tarefas de análise e descobrir padrões complexos em dados multidimensionais.